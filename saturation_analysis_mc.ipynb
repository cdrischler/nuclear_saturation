{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Bayesian analysis of the empirical saturation point (MC version)\n",
    "\n",
    "This notebook provides an independent implementation of our saturation analysis using brute-force Monte Carlo sampling. It can be used to check and generalize our analysis using conjugate priors.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import emcee as mc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numba as nb\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import invwishart, multivariate_normal, multivariate_t"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To test the analysis, let's create some dummy data from the prior we have used before:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "prior_params = {\"mu\": np.array([0.16, -15.9]),\n",
    "                \"Psi\": np.array([[0.01**2, 0], [0, 0.32**2]]),\n",
    "                \"kappa\": 1, \"nu\": 4}\n",
    "np.random.seed(42)\n",
    "tmp = multivariate_normal.rvs(mean=prior_params[\"mu\"], cov=prior_params[\"Psi\"], size=1000)\n",
    "data = pd.DataFrame(data={\"n0\": tmp[:, 0], \"E0\": tmp[:, 1]})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bayes' theorem\n",
    "\n",
    "Our analysis is based on Bayes' theorem. We need to define the prior, posterior, and likelihood. We use the (natural) logarithms of these quantities. The variables we will do the inference for are agglomerated in the parameter vector $\\theta = (\\mu_0, \\mu_1; \\Sigma_{0,0}, \\Sigma_{0,1}, \\Sigma_{1,1})$. To test the implementation, we define the $\\theta$ associated with the prior:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "theta_prior = (prior_params[\"mu\"][0], prior_params[\"mu\"][1],\n",
    "           prior_params[\"Psi\"][0, 0], prior_params[\"Psi\"][0, 1],\n",
    "           prior_params[\"Psi\"][1, 1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True, fastmath=True)\n",
    "def validate_matrix(mat, raise_error=False, atol_sym=1e-08, atol_eval=1e-06):\n",
    "    \"\"\"\n",
    "    Checks that the 2x2 matrix `mat` is symmetric and positive definite\n",
    "    :param mat: 2x2 matrix\n",
    "    :param raise_error: raise error if not symmetric and positive definite\n",
    "    :param atol_sym: absolute tolerance used for comparison (symmetry)\n",
    "    :param atol_eval: absolute tolerance used for comparison (positiveness)\n",
    "    :return: returns boolean result of the validation\n",
    "    \"\"\"\n",
    "    stat_sym = np.abs(mat[0,1]-mat[1,0]) < atol_sym\n",
    "    #v, _ = np.linalg.eig(mat) # only `eig` supported by numba\n",
    "    eigvals = 0.5 * (mat[0,0] - np.array([1., -1]) * np.sqrt(4.* mat[0,1]*mat[1,0] + (mat[0,0] - mat[1,1])**2) + mat[1,1])\n",
    "    stat_pos_def = (eigvals > atol_eval).all()\n",
    "    stat = stat_sym and stat_pos_def\n",
    "    if not stat and raise_error:\n",
    "        raise ValueError(\"Non-symmetric and/or non-positive-definite 2x2 matrix encountered.\")\n",
    "    else:\n",
    "        return stat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True)\n",
    "def parse_theta_vec(theta):\n",
    "    \"\"\"\n",
    "    converts the array/vector `theta` into a mean value and (symmetric) covariance matrix\n",
    "    :param theta: contains the components of the mean vector and the non-redundant components of the (symmetric) covariance matrix\n",
    "    :return: mean vector, full covariance matrix\n",
    "    \"\"\"\n",
    "    mu = np.array([theta[0], theta[1]])\n",
    "    cov = np.array([[theta[2], theta[3]], [theta[3], theta[4]]])\n",
    "    return mu, cov"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def log_prior(theta):\n",
    "    \"\"\"\n",
    "    computes the log pdf of our chosen prior distribution, the normal-inverse-Wishart distribution\n",
    "    Note: we can't use here `numba` straightforwardly because it does not support the distribution functions; `numba-scipy` and `numba-stats` address this issue but the inverse Wishart distribution has not been implemented in these libraries\n",
    "    :param theta: parameter vector\n",
    "    :return: log pdf associated with the prior\n",
    "    \"\"\"\n",
    "    mu, sigma = parse_theta_vec(theta)\n",
    "    if not validate_matrix(sigma):\n",
    "        return -np.inf\n",
    "    log_norm = multivariate_normal.logpdf(x=mu, mean=prior_params[\"mu\"], cov=sigma/prior_params[\"kappa\"])\n",
    "    log_invwishart = invwishart.logpdf(x=sigma, df=prior_params[\"nu\"], scale=prior_params[\"Psi\"])\n",
    "    return log_norm + log_invwishart"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import math\n",
    "def fast_log_2D_norm_invwishart(mu, Sigma, delta, gamma, Psi, alpha):\n",
    "    det_Sigma = -Sigma[0, 1] * Sigma[1, 0] + Sigma[0, 0] * Sigma[1, 1]\n",
    "    det_Psi = -Psi[0, 1] * Psi[1, 0] + Psi[0, 0] * Psi[1, 1]\n",
    "    Sigma_inv = np.array([[Sigma[1, 1], -Sigma[0, 1]], [-Sigma[1, 0], Sigma[0, 0]]]) / det_Sigma\n",
    "    invwishart_term = np.trace(Psi @ Sigma_inv)\n",
    "    diff = mu - delta\n",
    "    normal_term = gamma * np.dot(diff, np.dot(Sigma_inv, diff))\n",
    "    D = 2\n",
    "    norm_term = D / 2 * np.log(gamma)\n",
    "    norm_term += alpha / 2 * np.log(det_Psi)\n",
    "    norm_term -= (alpha + D + 2) / 2 * np.log(det_Sigma)\n",
    "    norm_term -= D / 2 * np.log(2. * np.pi)\n",
    "    norm_term -= alpha * D / 2 * np.log(2)\n",
    "    alpha_half = alpha / 2\n",
    "    norm_term -= 0.5 * np.log(np.pi) + np.log(math.gamma(alpha_half)) + np.log(math.gamma(alpha_half - 0.5))\n",
    "    return norm_term - 0.5 * (invwishart_term + normal_term)\n",
    "\n",
    "# def log_prior(theta):\n",
    "#     mu, sigma = parse_theta_vec(theta)\n",
    "#     if not validate_matrix(sigma):\n",
    "#         return -np.inf\n",
    "#     return fast_log_2D_norm_invwishart(mu=mu, Sigma=sigma, delta=prior_params[\"mu\"],\n",
    "#                                        gamma=prior_params[\"kappa\"], Psi=prior_params[\"Psi\"], alpha=prior_params[\"nu\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True, fastmath=True)\n",
    "def log_likelihood(theta, sat_data):\n",
    "    \"\"\"\n",
    "    computes the logarithm of the likelihood function, the standard Chi-squared likelihood function\n",
    "    :param theta: parameter vector\n",
    "    :param sat_data: array (number of point, 2) containing the datasets for the saturation density and energy\n",
    "    :return: logarithm of the likelihood function\n",
    "    \"\"\"\n",
    "    mu, sigma = parse_theta_vec(theta)\n",
    "    # exclude non-positive-definite covariance matrices\n",
    "    if not validate_matrix(sigma):\n",
    "        #print('covariance not valid')\n",
    "        return -np.inf\n",
    "    chi2 = 0.\n",
    "    sigma_inv = np.linalg.inv(sigma)\n",
    "    for row in sat_data:\n",
    "        diff = (row - mu)\n",
    "        chi2 += np.dot(diff, sigma_inv @ diff)\n",
    "    return -0.5 * (chi2 + len(sat_data) * np.log(np.abs(np.linalg.det(sigma))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def log_posterior(theta, sat_data):\n",
    "    \"\"\"\n",
    "    computes the log posterior, which is in this particular case also the normal-inverse-Wishart distribution (conjugagy).\n",
    "    :param theta: parameter vector\n",
    "    :param sat_data: pandas dataframe containing the datasets for the saturation density and energy\n",
    "    :return: log pdf associated with the posterior\n",
    "    \"\"\"\n",
    "    lp = log_prior(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + log_likelihood(theta, sat_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing our implementation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bayesian inference\n",
    "\n",
    "### Determine initial walker position using MLE or MAP\n",
    "\n",
    "We need to specify the initial position of the random walkers for Monte Carlo sampling. We use here either the MAP or MLE."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def get_initial_walker_pos(function=log_posterior, method=\"Nelder-Mead\", return_theta=False, **kwargs):\n",
    "    \"\"\"\n",
    "    finds the maximum of the argument `function`; to be used to determine either the maximum a posterior (MAP) or maximum likelihood estimation (MLE)\n",
    "    :param function: either likelihood function or posterior or prior\n",
    "    :param method: method used for optimization, see scipy documentation\n",
    "    :param kwargs: options passed to the optimizer, see scipy documentation\n",
    "    :param return_theta: boolean to request solution vector or (mu, cov)\n",
    "    :return: mean value and covariance associated with the found maximum or found maximum (if return_theta)\n",
    "    \"\"\"\n",
    "    nll = lambda *args: -function(*args)\n",
    "    args = () if function == log_prior else (data.to_numpy(),)\n",
    "    sol = minimize(nll, theta_prior, args=args, method=method, tol=1e-12, options=kwargs)\n",
    "    mu, cov = parse_theta_vec(sol.x)\n",
    "    if not validate_matrix(cov):\n",
    "        raise ValueError(\"Covariance matrix not positive definite\")\n",
    "    return sol.x if return_theta else (mu, cov)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "map_estimate = get_initial_walker_pos(function=log_posterior, method=\"Nelder-Mead\", maxiter=10000000)\n",
    "mle_estimate = get_initial_walker_pos(function=log_likelihood, method=\"SLSQP\", maxiter=10000000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We expect both the MAP and MLE to be close to the (input) prior, so let's compute the difference of the respective mean values and covariance matrices:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map\n",
      "[0.00056925 0.01060896]\n",
      "[[ 2.19399313e-06  8.00470285e-06]\n",
      " [ 8.00470285e-06 -8.45639101e-03]]\n",
      "estimated covariance matrix checks out: True\n",
      "mle\n",
      "[0.0005698  0.01061878]\n",
      "[[ 2.90476621e-06  8.08914970e-06]\n",
      " [ 8.08914970e-06 -7.80861025e-03]]\n",
      "estimated covariance matrix checks out: True\n"
     ]
    }
   ],
   "source": [
    "for lbl, est in ((\"map\", map_estimate), (\"mle\", mle_estimate)):\n",
    "    print(lbl)\n",
    "    mu, cov = est\n",
    "    print(mu-prior_params[\"mu\"])\n",
    "    print(cov-prior_params[\"Psi\"])\n",
    "    print(\"estimated covariance matrix checks out:\", validate_matrix(cov))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since the prior is quite uninformed, the MLE and MAP are very close to each other, as expected."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Monte Carlo sampling\n",
    "\n",
    "We use the library `multiprocess` because `multiprocessing` has issues with Jupyter notebooks.\n",
    "\n",
    "Warning: runtime might be several hours depending on the configuration."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import multiprocess as mp\n",
    "def run_mcmc_sampler(sat_data, nwalkers=1000, nsteps=50, nsteps_burn=50, log_prob_fn=log_posterior, num_threads=4, ndim=5, offset=1e-2):\n",
    "    \"\"\"\n",
    "    samples the function provided by `log_prob_fn`\n",
    "    :param sat_data: datasets for saturation point\n",
    "    :param nwalkers: number of walkers\n",
    "    :param nsteps: number of MCMC steps after burn-in phase\n",
    "    :param nsteps_burn: number of MCMC steps used for burn in\n",
    "    :param log_prob_fn: function to be sampled\n",
    "    :param num_threads: number of threads to be used\n",
    "    :param ndim: dimension of parameter vector `theta` (here: 5)\n",
    "    :param offset: magnitude of random displacement from walkers' initial positions\n",
    "    :return: sampler, pos, prob, state (see emcee documentation)\n",
    "    \"\"\"\n",
    "    if log_prob_fn == log_likelihood:\n",
    "        init_pos = get_initial_walker_pos(function=log_prob_fn, method=\"SLSQP\",\n",
    "        return_theta=True)\n",
    "    else:\n",
    "        init_pos = get_initial_walker_pos(function=log_posterior, method=\"Nelder-Mead\", return_theta=True)\n",
    "\n",
    "    if num_threads is None:\n",
    "        num_threads = mp.cpu_count()\n",
    "\n",
    "    args = None if log_prob_fn == log_prior else (sat_data.to_numpy(),)\n",
    "    with mp.Pool(num_threads) as pool:\n",
    "        # for multiprocessing with emcee, see https://emcee.readthedocs.io/en/stable/tutorials/parallel/#multiprocessing\n",
    "        # https://stackoverflow.com/questions/41385708/multiprocessing-example-giving-attributeerror\n",
    "        sampler = mc.EnsembleSampler(nwalkers=nwalkers, ndim=ndim,\n",
    "                                     log_prob_fn=log_prob_fn, args=args, pool=pool)\n",
    "        pos = init_pos + offset * np.random.randn(nwalkers, ndim)\n",
    "        if nsteps_burn > 0:\n",
    "            pos, _, _ = sampler.run_mcmc(initial_state=pos, nsteps=nsteps_burn, progress=True)\n",
    "            sampler.reset()\n",
    "        pos, prob, state = sampler.run_mcmc(initial_state=pos, nsteps=nsteps, progress=True)\n",
    "    return sampler, pos, prob, state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing\n",
    "\n",
    "We test here that the implemented distribution functions match the analytically known results for the mean values.\n",
    "\n",
    "We begin the testing with the prior distribution:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]/Users/cdrischler/Dropbox/Projects/nuclear_saturation/env_satpoint/lib/python3.9/site-packages/emcee/moves/red_blue.py:99: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  lnpdiff = f + nlp - state.log_prob[j]\n",
      "100%|██████████| 50000/50000 [06:40<00:00, 124.73it/s]\n",
      "100%|██████████| 500000/500000 [1:01:01<00:00, 136.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([ 1.60021915e-01, -1.59003235e+01,  1.17552080e-04, -3.76557430e-04,\n        1.13676823e-01])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler, pos, prob, state = run_mcmc_sampler(sat_data=data, log_prob_fn=log_prior, nwalkers=10, nsteps=500000, nsteps_burn=50000)\n",
    "samples_prior = sampler.flatchain\n",
    "# sampler.chain[:,:,4].mean(axis=1)\n",
    "samples_prior.mean(axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The expected covariance of the two components of the mean vector is $\\Psi / \\kappa$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1.58811247e-05, -5.34573594e-04],\n       [-5.34573594e-04,  1.88727468e-02]])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#samples#.mean(axis=0)\n",
    "expected = prior_params[\"Psi\"]/prior_params[\"kappa\"]\n",
    "got = np.cov(samples_prior[:,0], samples_prior[:,1])\n",
    "got - expected"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our expectation for the mean values from the prior distribution is a follows:\n",
    "$$\n",
    "E(\\mu) = \\mu_0 \\;, \\qquad\n",
    "E(\\Sigma) = \\frac{\\Sigma}{\\nu - p -1} \\quad \\mathrm{for} \\; \\nu > p + 1\n",
    "$$\n",
    "with $p=2$ being associated with rank of the $p \\times p$ covariance matrix $\\Sigma$. Hence, the mean value of the covariance matrix is _undefined_ in the original case we considered, $\\nu_0 = 3$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def mean_val_inv_wish(Psi, nu):\n",
    "    \"\"\"\n",
    "    computes the mean value of the Inverse-Wishart distribution, see https://en.wikipedia.org/wiki/Inverse-Wishart_distribution\n",
    "    :param Lambda: covariance matrix\n",
    "    :param nu: degree of freedom\n",
    "    :return: mean value of the associated Inverse-Wishart distribution\n",
    "    \"\"\"\n",
    "    p = len(Psi)\n",
    "    mean = Psi / (nu - p - 1) if nu > p+1 else np.inf\n",
    "    mode = Psi / (nu + p + 1)\n",
    "\n",
    "    tmp = nu - p - 1\n",
    "    cov = np.zeros((4,4))\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ii = i + 2*j\n",
    "            for k in range(2):\n",
    "                for l in range(2):\n",
    "                    jj = k + 2*l\n",
    "                    cov[ii, jj] = 2 * Psi[i,j]*Psi[k,l] + tmp * (Psi[i,k]*Psi[j,l] + Psi[i,l]*Psi[k,j])\n",
    "                    # taken from https://en.wikipedia.org/wiki/Inverse-Wishart_distribution#Moments\n",
    "    cov = cov / (nu-p)*tmp**2*(nu-p-3)\n",
    "    print(\"covariance matrix\", cov)\n",
    "    return mean, mode"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariance matrix [[-2.000000e-08 -0.000000e+00 -0.000000e+00 -1.024000e-05]\n",
      " [-0.000000e+00 -5.120000e-06 -5.120000e-06 -0.000000e+00]\n",
      " [-0.000000e+00 -5.120000e-06 -5.120000e-06 -0.000000e+00]\n",
      " [-1.024000e-05 -0.000000e+00 -0.000000e+00 -2.097152e-02]]\n",
      "mean of mean vector: [  0.16 -15.9 ]\n",
      "mean of covariance matrix: [[1.000e-04 0.000e+00]\n",
      " [0.000e+00 1.024e-01]]\n",
      "mode of covariance matrix: [[1.42857143e-05 0.00000000e+00]\n",
      " [0.00000000e+00 1.46285714e-02]]\n"
     ]
    }
   ],
   "source": [
    "mean_mean_vec = prior_params[\"mu\"]\n",
    "mean_cov, mode_cov = mean_val_inv_wish(prior_params[\"Psi\"], prior_params[\"nu\"])\n",
    "print(f\"mean of mean vector: {mean_mean_vec}\")\n",
    "print(f\"mean of covariance matrix: {mean_cov}\")\n",
    "print(f\"mode of covariance matrix: {mode_cov}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The mean vector and covariance matrix seem to be in agreement with the ones from the prior.\n",
    "Let's investigate the mode of the prior, which seems slightly off. Why? Because the prior is relatively flat?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([  0.16, -15.9 ]),\n array([[ 1.24999999e-05, -1.51130086e-11],\n        [-1.51130086e-11,  1.27999999e-02]]))"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_initial_walker_pos(function=log_prior, method=\"Nelder-Mead\", maxiter=10000000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 6.51419682e-06, -4.44283713e-05,  1.35398518e-03],\n       [-4.44283713e-05,  1.36134562e-03, -7.61832907e-02],\n       [ 1.35398518e-03, -7.61832907e-02,  5.61022753e+00]])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov((samples_prior[:,2], samples_prior[:,3], samples_prior[:,4]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Why is the last component so large?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's investigate the predictive $y$ distribution:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def predictive_y(samples, size=1):\n",
    "    y_samples = []\n",
    "    for row in samples:\n",
    "        mu, Sigma = parse_theta_vec(row)\n",
    "        y_samples.append(multivariate_normal.rvs(mean=mu, cov=Sigma, size=size))\n",
    "    return np.array(y_samples)\n",
    "pred_y_samples = predictive_y(samples_prior)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the mean value of the prior predictive $y$, which is the Student's $t$-distribution, we expect just the prior mean vector. We find agreement with the expectation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "array([  0.16001852, -15.90056876])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y_samples.mean(axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We expect\n",
    "\\begin{equation}\n",
    "    P(\\boldsymbol{y}^*| \\mathcal{D}) \\sim t_{\\nu_n -p +1} \\Big( \\boldsymbol{\\mu_n}, \\frac{\\boldsymbol{\\Lambda_n}(k_n+1)}{k_n(\\nu_n-p+1)} \\Big),\n",
    "\\end{equation}\n",
    "so the mean vector is just the mean vector of the prior and the variance\n",
    "$$\n",
    "\\frac{\\nu'_n}{\\nu'_n-2} \\boldsymbol{\\Sigma}' =  \\frac{\\boldsymbol{\\Lambda_n}(k_n+1)}{k_n(\\nu_n-p-1)}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[2.000e-04, 0.000e+00],\n       [0.000e+00, 2.048e-01]])"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def variance_multivar_t(nu, Sigma):\n",
    "    # see https://en.wikipedia.org/wiki/Multivariate_t-distribution\n",
    "    return nu/(nu-2) * Sigma if nu > 2 else np.inf\n",
    "variance_multivar_t(prior_params[\"nu\"]-2+1,\n",
    "                    prior_params[\"Psi\"] * (prior_params[\"kappa\"]+1)/(prior_params[\"kappa\"]*(prior_params[\"nu\"]-2+1)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 2.30281004e-04, -8.77585530e-04],\n       [-8.77585530e-04,  2.31747522e-01]])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov(pred_y_samples[:,0], pred_y_samples[:,1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It looks like the variances agree (although not perfectly). Why? Do we just need more sampling points? Prior too uninformed?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, let's consider the posterior distribution:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [12:41<00:00, 131.24it/s]\n",
      "100%|██████████| 300000/300000 [37:40<00:00, 132.69it/s]\n"
     ]
    }
   ],
   "source": [
    "sampler, pos, prob, state = run_mcmc_sampler(sat_data=data, log_prob_fn=log_posterior, nwalkers=10, nsteps=300000, nsteps_burn=100000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 1.61212812e-01, -1.58892717e+01, -4.68066723e-03, -1.79985087e-04,\n        9.44153713e-02])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_posterior = sampler.flatchain\n",
    "samples_posterior.mean(axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our expectation for the mean values from the updated prior distribution due to conjugacy is as follows (see our manuscript for the equations):"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected mean vector [  0.16056925 -15.88939104]\n",
      "covariance matrix [[2.12439583e+07 1.66399881e+06 1.66399881e+06 1.96201072e+07]\n",
      " [1.66399881e+06 9.75475991e+09 9.75475991e+09 1.52965993e+09]\n",
      " [1.66399881e+06 9.75475991e+09 9.75475991e+09 1.52965993e+09]\n",
      " [1.96201072e+07 1.52965993e+09 1.52965993e+09 1.79522629e+13]]\n",
      "expected covariance matrix: [[1.02908636e-04 8.06063757e-06]\n",
      " [8.06063757e-06 9.46005591e-02]]\n"
     ]
    }
   ],
   "source": [
    "ret = dict()\n",
    "ret[\"kappa\"] = prior_params[\"kappa\"] + len(data)\n",
    "ret[\"nu\"] = prior_params[\"nu\"] + len(data)\n",
    "ret[\"sample_mean\"] = data.mean().to_numpy()\n",
    "ret[\"mu\"] = np.array([prior_params[\"kappa\"], len(data)]) @ np.array([prior_params[\"mu\"], ret[\"sample_mean\"]]) / ret[\"kappa\"]\n",
    "diff = ret[\"sample_mean\"] - prior_params[\"mu\"]\n",
    "tmp = data - ret[\"sample_mean\"]\n",
    "ret[\"sum_squared_dev\"] = np.sum([np.outer(row.to_numpy(), row.to_numpy()) for irow, row in tmp.iterrows()], axis=0)\n",
    "ret[\"Psi\"] = prior_params[\"Psi\"] + ret[\"sum_squared_dev\"] + (prior_params[\"kappa\"] * len(data) / ret[\"kappa\"]) * np.outer(diff, diff)\n",
    "print(\"expected mean vector\", ret[\"mu\"])\n",
    "print(\"expected covariance matrix:\", mean_val_inv_wish(ret[\"Psi\"], ret[\"nu\"])[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that $\\Sigma_{1,1}$ (lower right) is large \\[Note that the matrix elements above include the denominator from the mean value\\]. Does that make sense? But keep in mind that $\\Sigma/\\kappa_n$ will be used as covariance matrix in the normal distribution (of the Normal-Inverse Wishart)---but not in the inverse Wishart distribution.\n",
    "\n",
    "The mean vector looks like a match. Except for the $\\Sigma_{11}$ component, it does not look like a good match for $\\Sigma$. Here are the ratios of the matrix elements:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariance matrix [[2.12439583e+07 1.66399881e+06 1.66399881e+06 1.96201072e+07]\n",
      " [1.66399881e+06 9.75475991e+09 9.75475991e+09 1.52965993e+09]\n",
      " [1.66399881e+06 9.75475991e+09 9.75475991e+09 1.52965993e+09]\n",
      " [1.96201072e+07 1.52965993e+09 1.52965993e+09 1.79522629e+13]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[-45.48371633, -22.32888973],\n       [-22.32888973,   0.99804242]])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m, c = parse_theta_vec(samples_posterior.mean(axis=0))\n",
    "c/mean_val_inv_wish(ret[\"Psi\"], ret[\"nu\"])[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-8.15492565e-05, -1.74478068e-05],\n       [-1.74478068e-05, -9.45227281e-02]])"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#samples#.mean(axis=0)\n",
    "expected = ret[\"Psi\"]/ret[\"kappa\"]\n",
    "got = np.cov(samples_posterior[:,0], samples_posterior[:,1])\n",
    "got - expected"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariance matrix [[2.12439583e+07 1.66399881e+06 1.66399881e+06 1.96201072e+07]\n",
      " [1.66399881e+06 9.75475991e+09 9.75475991e+09 1.52965993e+09]\n",
      " [1.66399881e+06 9.75475991e+09 9.75475991e+09 1.52965993e+09]\n",
      " [1.96201072e+07 1.52965993e+09 1.52965993e+09 1.79522629e+13]]\n",
      "mean of mean vector: [  0.16056925 -15.88939104]\n",
      "mean of covariance matrix: [[1.02908636e-04 8.06063757e-06]\n",
      " [8.06063757e-06 9.46005591e-02]]\n",
      "mode of covariance matrix: [[1.02295477e-04 8.01260993e-06]\n",
      " [8.01260993e-06 9.40369013e-02]]\n"
     ]
    }
   ],
   "source": [
    "mean_mean_vec = ret[\"mu\"]\n",
    "mean_cov, mode_cov = mean_val_inv_wish(ret[\"Psi\"], ret[\"nu\"])\n",
    "print(f\"mean of mean vector: {mean_mean_vec}\")\n",
    "print(f\"mean of covariance matrix: {mean_cov}\")\n",
    "print(f\"mode of covariance matrix: {mode_cov}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([  0.16056925, -15.88939104]),\n array([[1.02193993e-04, 8.00470285e-06],\n        [8.00470285e-06, 9.39436090e-02]]))"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_initial_walker_pos(function=log_posterior, method=\"Nelder-Mead\", maxiter=10000000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[5.72091366e-05, 5.84218209e-06, 8.61286205e-06],\n       [5.84218209e-06, 5.46795449e-06, 4.55673981e-06],\n       [8.61286205e-06, 4.55673981e-06, 2.29364844e-05]])"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov((samples_posterior[:, 2], samples_posterior[:, 3], samples_posterior[:, 4]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cdrischler/Dropbox/Projects/nuclear_saturation/env_satpoint/lib/python3.9/site-packages/scipy/stats/_multivariate.py:653: RuntimeWarning: covariance is not positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([6.05525704e-04, 7.15860662e-05])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y_samples = predictive_y(samples_posterior)\n",
    "pred_y_samples.mean(axis=0) - ret[\"mu\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.03011442e-04, 8.06869015e-06],\n       [8.06869015e-06, 9.46950651e-02]])"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variance_multivar_t(ret[\"nu\"] - 2 + 1,\n",
    "                    ret[\"Psi\"] * (ret[\"kappa\"] + 1) / (ret[\"kappa\"] * (ret[\"nu\"] - 2 + 1)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 4.93853201e-03, -6.90185718e-05],\n       [-6.90185718e-05,  9.46341335e-02]])"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov(pred_y_samples[:, 0], pred_y_samples[:, 1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Useful links\n",
    "\n",
    "* [Covariances and Pearson correlation](https://cxwangyi.wordpress.com/2010/08/29/pearson-correlation-coefficient-covariance-matrix-and-linear-dependency/)\n",
    "* [emcee tutorial for fitting (1)](https://prappleizer.github.io/Tutorials/MCMC/MCMC_Tutorial_Solution.html)\n",
    "* [emcee tutorial for fitting (2)](https://users.obs.carnegiescience.edu/cburns/ipynbs/Emcee.html)\n",
    "* [emcee tutorial for fitting (3)](https://emcee.readthedocs.io/en/stable/tutorials/line/)\n",
    "* [multivariate normal distribution](https://online.stat.psu.edu/stat505/book/export/html/636)\n",
    "* [A Note on Wishart and Inverse Wishart Priors for Covariance Matrix](https://jbds.isdsa.org/public/journals/1/html/v1n2/p2/)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "satpoint",
   "language": "python",
   "display_name": "satpoint"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}