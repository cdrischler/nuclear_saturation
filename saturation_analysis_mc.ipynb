{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# A Bayesian mixture model approach to quantifying the empirical nuclear saturation point (simplified MC version)\n",
    "\n",
    "This notebook provides the starting point for an independent implementation of our saturation analysis using brute-force Monte Carlo sampling. It could be used to check and generalize our analysis based on conjugate distributions. This notebook was not used in our analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import emcee as mc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numba as nb\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import invwishart, multivariate_normal, multivariate_t\n",
    "from modules.plot_helpers import plot_confregion_bivariate_t\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To test the analysis, let's create some dummy data from the prior we have used before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prior_params = {\"mu\": np.array([0.16, -15.9]),\n",
    "                \"Psi\": np.array([[0.01**2, 0], [0, 0.32**2]]),  # upper triangle will be ignored\n",
    "                \"kappa\": 1, \"nu\": 4}\n",
    "np.random.seed(42)\n",
    "tmp = multivariate_normal.rvs(mean=prior_params[\"mu\"], cov=prior_params[\"Psi\"], size=1000)\n",
    "data = pd.DataFrame(data={\"n0\": tmp[:, 0], \"E0\": tmp[:, 1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Bayes' theorem\n",
    "\n",
    "Our analysis is based on Bayes' theorem. We need to define the prior, posterior, and likelihood. We use the logarithms of these quantities. The variables we will do the inference for are agglomerated in the parameter vector $\\theta = (\\mu_0, \\mu_1; \\Sigma_{0,0}, \\Sigma_{0,1}, \\Sigma_{1,1})$. To test the implementation, we define the $\\theta$ associated with the prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "theta_prior = (prior_params[\"mu\"][0], prior_params[\"mu\"][1],\n",
    "           prior_params[\"Psi\"][0, 0], prior_params[\"Psi\"][0, 1],\n",
    "           prior_params[\"Psi\"][1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True, fastmath=True)\n",
    "def validate_matrix(mat, raise_error=False, atol_sym=1e-08, atol_eval=1e-06):\n",
    "    \"\"\"\n",
    "    Checks that the 2x2 matrix `mat` is symmetric and positive definite\n",
    "    :param mat: 2x2 matrix\n",
    "    :param raise_error: raise error if not symmetric and positive definite\n",
    "    :param atol_sym: absolute tolerance used for comparison (symmetry)\n",
    "    :param atol_eval: absolute tolerance used for comparison (positiveness)\n",
    "    :return: returns boolean result of the validation\n",
    "    \"\"\"\n",
    "    stat_sym = np.abs(mat[0,1]-mat[1,0]) < atol_sym\n",
    "    #v, _ = np.linalg.eig(mat) # only `eig` supported by numba\n",
    "    eigvals = 0.5 * (mat[0,0] - np.array([1., -1]) * np.sqrt(4.* mat[0,1]*mat[1,0] + (mat[0,0] - mat[1,1])**2) + mat[1,1])\n",
    "    stat_pos_def = (eigvals > atol_eval).all()\n",
    "    stat = stat_sym and stat_pos_def\n",
    "    if not stat and raise_error:\n",
    "        raise ValueError(\"Non-symmetric and/or non-positive-definite 2x2 matrix encountered.\")\n",
    "    else:\n",
    "        return stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True)\n",
    "def parse_theta_vec(theta):\n",
    "    \"\"\"\n",
    "    converts the array/vector `theta` into a mean value and (symmetric) covariance matrix\n",
    "    :param theta: encodes mean vector (first two components) and the (symmetric) covariance matrix (last 3 components);\n",
    "    :return: mean vector, full covariance matrix\n",
    "    \"\"\"\n",
    "    mu = np.array([theta[0], theta[1]])\n",
    "    cov = np.array([[theta[2], theta[3]], [theta[3], theta[4]]])\n",
    "     \n",
    "    # One might think of using the Choleslky decomposition to enforce positive (semi-)definitess of the returned matrix\n",
    "    ## cov[0,1]=0 # treat components as lower triangle matrix\n",
    "    ## cov = cov @ cov.T # use Cholesky decomposition to ensure at at least positive semi-definiteness\n",
    "    # but this causes issues with the MLE and MAP solvers below\n",
    "    \n",
    "    return mu, cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def log_prior(theta):\n",
    "    \"\"\"\n",
    "    computes the log pdf of our chosen prior distribution, the normal-inverse-Wishart distribution\n",
    "    Note: we can't use here `numba` straightforwardly because it does not support the distribution functions; `numba-scipy` and `numba-stats` address this issue but the inverse Wishart distribution has not been implemented in these libraries\n",
    "    :param theta: parameter vector\n",
    "    :return: log pdf associated with the prior\n",
    "    \"\"\"\n",
    "    mu, sigma = parse_theta_vec(theta)\n",
    "    if not validate_matrix(sigma):\n",
    "        return -np.inf\n",
    "    log_norm = multivariate_normal.logpdf(x=mu, mean=prior_params[\"mu\"], cov=sigma/prior_params[\"kappa\"])\n",
    "    log_invwishart = invwishart.logpdf(x=sigma, df=prior_params[\"nu\"], scale=prior_params[\"Psi\"])\n",
    "    return log_norm + log_invwishart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def fast_log_2D_norm_invwishart(mu, Sigma, delta, gamma, Psi, alpha):\n",
    "    det_Sigma = -Sigma[0, 1] * Sigma[1, 0] + Sigma[0, 0] * Sigma[1, 1]\n",
    "    det_Psi = -Psi[0, 1] * Psi[1, 0] + Psi[0, 0] * Psi[1, 1]\n",
    "    Sigma_inv = np.array([[Sigma[1, 1], -Sigma[0, 1]], [-Sigma[1, 0], Sigma[0, 0]]]) / det_Sigma\n",
    "    invwishart_term = np.trace(Psi @ Sigma_inv)\n",
    "    diff = mu - delta\n",
    "    normal_term = gamma * np.dot(diff, np.dot(Sigma_inv, diff))\n",
    "    D = 2\n",
    "    norm_term = D / 2 * np.log(gamma)\n",
    "    norm_term += alpha / 2 * np.log(det_Psi)\n",
    "    norm_term -= (alpha + D + 2) / 2 * np.log(det_Sigma)\n",
    "    norm_term -= D / 2 * np.log(2. * np.pi)\n",
    "    norm_term -= alpha * D / 2 * np.log(2)\n",
    "    alpha_half = alpha / 2\n",
    "    norm_term -= 0.5 * np.log(np.pi) + np.log(math.gamma(alpha_half)) + np.log(math.gamma(alpha_half - 0.5))\n",
    "    return norm_term - 0.5 * (invwishart_term + normal_term)\n",
    "\n",
    "# def log_prior(theta):\n",
    "#     mu, sigma = parse_theta_vec(theta)\n",
    "#     if not validate_matrix(sigma):\n",
    "#         return -np.inf\n",
    "#     return fast_log_2D_norm_invwishart(mu=mu, Sigma=sigma, delta=prior_params[\"mu\"],\n",
    "#                                        gamma=prior_params[\"kappa\"], Psi=prior_params[\"Psi\"], alpha=prior_params[\"nu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True, fastmath=True)\n",
    "def log_likelihood(theta, sat_data):\n",
    "    \"\"\"\n",
    "    computes the logarithm of the likelihood function, the standard Chi-squared likelihood function\n",
    "    :param theta: parameter vector\n",
    "    :param sat_data: array (number of point, 2) containing the datasets for the saturation density and energy\n",
    "    :return: logarithm of the likelihood function\n",
    "    \"\"\"\n",
    "    mu, sigma = parse_theta_vec(theta)\n",
    "    # exclude non-positive-definite covariance matrices\n",
    "    if not validate_matrix(sigma):\n",
    "        #print('covariance not valid')\n",
    "        return -np.inf\n",
    "    chi2 = 0.\n",
    "    sigma_inv = np.linalg.inv(sigma)\n",
    "    for row in sat_data:\n",
    "        diff = (row - mu)\n",
    "        chi2 += np.dot(diff, sigma_inv @ diff)\n",
    "    return -0.5 * (chi2 + len(sat_data) * np.log(np.abs(np.linalg.det(sigma))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def log_posterior(theta, sat_data):\n",
    "    \"\"\"\n",
    "    computes the log posterior, which is in this particular case also the normal-inverse-Wishart distribution (conjugagy).\n",
    "    :param theta: parameter vector\n",
    "    :param sat_data: pandas dataframe containing the datasets for the saturation density and energy\n",
    "    :return: log pdf associated with the posterior\n",
    "    \"\"\"\n",
    "    lp = log_prior(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + log_likelihood(theta, sat_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Testing our implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Bayesian inference\n",
    "\n",
    "### Determine initial walker position using MLE or MAP\n",
    "\n",
    "We need to specify the initial position of the random walkers for Monte Carlo sampling. We use here either the MAP or MLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_initial_walker_pos(function=log_posterior, method=\"Nelder-Mead\", return_theta=False, **kwargs):\n",
    "    \"\"\"\n",
    "    finds the maximum of the argument `function`; to be used to determine either the maximum a posterior (MAP) or maximum likelihood estimation (MLE)\n",
    "    :param function: either likelihood function or posterior or prior\n",
    "    :param method: method used for optimization, see scipy documentation\n",
    "    :param kwargs: options passed to the optimizer, see scipy documentation\n",
    "    :param return_theta: boolean to request solution vector or (mu, cov)\n",
    "    :return: mean value and covariance associated with the found maximum or found maximum (if return_theta)\n",
    "    \"\"\"\n",
    "    nll = lambda *args: -function(*args)\n",
    "    args = () if function == log_prior else (data.to_numpy(),)\n",
    "    sol = minimize(nll, theta_prior, args=args, method=method, tol=1e-12, options=kwargs)\n",
    "    mu, cov = parse_theta_vec(sol.x)\n",
    "    if not validate_matrix(cov):\n",
    "        raise ValueError(\"Covariance matrix not positive definite\")\n",
    "    return sol.x if return_theta else (mu, cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "map_estimate = get_initial_walker_pos(function=log_posterior, method=\"Nelder-Mead\", maxiter=10000000)\n",
    "mle_estimate = get_initial_walker_pos(function=log_likelihood, method=\"SLSQP\", maxiter=10000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We expect both the MAP and MLE to be close to the (input) prior, so let's compute the difference of the respective mean values and covariance matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map\n",
      "[0.00056925 0.01060896]\n",
      "[[ 2.19399313e-06  8.00470285e-06]\n",
      " [ 8.00470285e-06 -8.45639101e-03]]\n",
      "estimated covariance matrix checks out: True\n",
      "mle\n",
      "[0.0005698  0.01061878]\n",
      "[[ 2.90476621e-06  8.08914970e-06]\n",
      " [ 8.08914970e-06 -7.80861025e-03]]\n",
      "estimated covariance matrix checks out: True\n"
     ]
    }
   ],
   "source": [
    "for lbl, est in ((\"map\", map_estimate), (\"mle\", mle_estimate)):\n",
    "    print(lbl)\n",
    "    mu, cov = est\n",
    "    print(mu-prior_params[\"mu\"])\n",
    "    print(cov-prior_params[\"Psi\"])\n",
    "    print(\"estimated covariance matrix checks out:\", validate_matrix(cov))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since the prior is only weakly informed, the MLE and MAP are very close to each other, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Monte Carlo sampling\n",
    "\n",
    "We use the library `multiprocess` because `multiprocessing` has issues with Jupyter notebooks.\n",
    "\n",
    "Warning: runtime might be several hours depending on the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_init_pos_walkers(init_pos, nwalkers, enforce_posdef=True, std_perturbation=1e-2, delta_mean=0.02):\n",
    "    \"\"\"\n",
    "    creates initial positions of `nwalkers` by randomly perturbing a given (single) initial position `init_pos` \n",
    "    :param init_pos: datasets for saturation point\n",
    "    :param nwalkers: number of walkers requested\n",
    "    :param enforce_posdef: enforce that matrix component in the output is positive (semi-)definite (in addition to symmetric)\n",
    "    :param std_perturbation: standard deviation of the perturbation\n",
    "    :param delta_mean: mean value of the normal distribution (delta_mean, std_perturbation**2) for the minimum eigenvalues used to obtain the nearest pos. def. matrix in Frobenius norm\n",
    "    :return: array of shape (nwalkers, len(init_pos)) containing the perturbed initial positions of the random walkers\n",
    "    \"\"\"\n",
    "    # perturbation preserves symmetry but not definiteness of matrix `init_pos`\n",
    "    # return array will be component-wise distributed as \"Normal(mu=init_pos, sigma^2=std_perturbation)\"\n",
    "    ret = init_pos + std_perturbation * np.random.randn(nwalkers, len(init_pos)) \n",
    "    \n",
    "    # make matrix encoded in `init_pos` (i.e., theta) positive (semi-)definite after perturbation,\n",
    "    # by finding the nearest sym. pos. def. matrix in Frobenius norm\n",
    "    # https://nhigham.com/2021/01/26/what-is-the-nearest-positive-semidefinite-matrix/ (based on Cheng & Higham, 1998)\n",
    "    if enforce_posdef:\n",
    "        triu_indices = np.triu_indices(2)\n",
    "        for row in ret:\n",
    "            mu, mat = parse_theta_vec(row)\n",
    "            if not validate_matrix(mat): # leave walker alone if its matrix is already pos. def.\n",
    "                v, w = np.linalg.eig(mat)\n",
    "                delta = np.abs(delta_mean+std_perturbation * np.random.randn())\n",
    "                cov = w @ np.diag(np.clip(v, a_min=delta, a_max=None)) @ w.T  # increase eigenvalues if necessary\n",
    "                validate_matrix(cov, raise_error=True)\n",
    "                row[2:] = cov[triu_indices]\n",
    "    return ret\n",
    "# perturb_init_pos_walkers([1,1,2,3,4], 5)\n",
    "# perturb_init_pos_walkers([1,1,1,0,0.001], 5, enforce_posdef=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocess as mp\n",
    "def run_mcmc_sampler(sat_data, nwalkers=1000, nsteps=50, nsteps_burn=50, log_prob_fn=log_posterior, num_threads=4, offset=1e-2):\n",
    "    \"\"\"\n",
    "    samples the function provided by `log_prob_fn`\n",
    "    :param sat_data: datasets for saturation point\n",
    "    :param nwalkers: number of walkers\n",
    "    :param nsteps: number of MCMC steps after burn-in phase\n",
    "    :param nsteps_burn: number of MCMC steps used for burn in\n",
    "    :param log_prob_fn: function to be sampled\n",
    "    :param num_threads: number of threads to be used\n",
    "    :param offset: magnitude of random displacement from walkers' initial positions\n",
    "    :return: sampler, pos, prob, state (see emcee documentation)\n",
    "    \"\"\"\n",
    "    if log_prob_fn == log_likelihood:\n",
    "        init_pos = get_initial_walker_pos(function=log_prob_fn, method=\"SLSQP\",\n",
    "        return_theta=True)\n",
    "    else:\n",
    "        init_pos = get_initial_walker_pos(function=log_posterior, method=\"Nelder-Mead\", return_theta=True)\n",
    "\n",
    "    if num_threads is None:\n",
    "        num_threads = mp.cpu_count()\n",
    "\n",
    "    ndim = len(init_pos)\n",
    "    args = None if log_prob_fn == log_prior else (sat_data.to_numpy(),)\n",
    "    with mp.Pool(num_threads) as pool:\n",
    "        # for multiprocessing with emcee, see https://emcee.readthedocs.io/en/stable/tutorials/parallel/#multiprocessing\n",
    "        # https://stackoverflow.com/questions/41385708/multiprocessing-example-giving-attributeerror\n",
    "        sampler = mc.EnsembleSampler(nwalkers=nwalkers, ndim=ndim,\n",
    "                                     log_prob_fn=log_prob_fn, args=args, pool=pool)\n",
    "        \n",
    "        # let each walker start at a slightly different initial position (random perturbation)\n",
    "        pos = perturb_init_pos_walkers(init_pos, nwalkers, enforce_posdef=True, std_perturbation=offset)\n",
    "    \n",
    "        if nsteps_burn > 0:\n",
    "            pos, _, _ = sampler.run_mcmc(initial_state=pos, nsteps=nsteps_burn, progress=True)\n",
    "            sampler.reset()\n",
    "        pos, prob, state = sampler.run_mcmc(initial_state=pos, nsteps=nsteps, progress=True)\n",
    "    return sampler, pos, prob, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Testing\n",
    "\n",
    "We test here that the implemented distribution functions match the analytically known results for the mean values.\n",
    "\n",
    "We begin the testing with the **prior distribution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:39<00:00, 125.16it/s]\n",
      "100%|██████████| 50000/50000 [06:46<00:00, 122.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.59939427e-01, -1.59030851e+01,  8.62527648e-05, -4.59882518e-05,\n",
       "        1.04112204e-01])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler, pos, prob, state = run_mcmc_sampler(sat_data=data, log_prob_fn=log_prior, nwalkers=10, nsteps=100000, nsteps_burn=5000)\n",
    "samples_prior = sampler.flatchain\n",
    "# sampler.chain[:,:,4].mean(axis=1)\n",
    "samples_prior.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can compare the estimated mean vector and covariance matrix with the expected values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.05727478e-05, -3.08505565e-03])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_prior.mean(axis=0)[:2] - prior_params[\"mu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.38596595e-05,  1.02884100e-04],\n",
       "       [ 1.02884100e-04,  1.97642242e-02]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est_cov = np.cov(samples_prior[:,0], samples_prior[:,1])\n",
    "nu = prior_params[\"nu\"] - 2 + 1\n",
    "pref = nu / (nu - 2)\n",
    "est_cov - pref * prior_params[\"Psi\"]/(prior_params[\"kappa\"]*nu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Useful links\n",
    "\n",
    "* [Covariances and Pearson correlation](https://cxwangyi.wordpress.com/2010/08/29/pearson-correlation-coefficient-covariance-matrix-and-linear-dependency/)\n",
    "* [emcee tutorial for fitting (1)](https://prappleizer.github.io/Tutorials/MCMC/MCMC_Tutorial_Solution.html)\n",
    "* [emcee tutorial for fitting (2)](https://users.obs.carnegiescience.edu/cburns/ipynbs/Emcee.html)\n",
    "* [emcee tutorial for fitting (3)](https://emcee.readthedocs.io/en/stable/tutorials/line/)\n",
    "* [multivariate normal distribution](https://online.stat.psu.edu/stat505/book/export/html/636)\n",
    "* [A Note on Wishart and Inverse Wishart Priors for Covariance Matrix](https://jbds.isdsa.org/public/journals/1/html/v1n2/p2/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_satpoint_mc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
