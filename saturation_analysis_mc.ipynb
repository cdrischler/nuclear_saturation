{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Bayesian analysis of the empirical saturation point (MC version)\n",
    "\n",
    "This notebook provides an independent implementation of our saturation analysis using brute-force Monte Carlo sampling. It can be used to check and generalize our analysis using conjugate priors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import emcee as mc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numba as nb\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import invwishart, multivariate_normal, multivariate_t\n",
    "from modules.plot_helpers import plot_confregion_bivariate_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To test the analysis, let's create some dummy data from the prior we have used before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prior_params = {\"mu\": np.array([0.16, -15.9]),\n",
    "                \"Psi\": np.array([[0.01**2, 0], [0, 0.32**2]]),  # upper triangle will be ignored\n",
    "                \"kappa\": 1, \"nu\": 4}\n",
    "np.random.seed(42)\n",
    "tmp = multivariate_normal.rvs(mean=prior_params[\"mu\"], cov=prior_params[\"Psi\"], size=1000)\n",
    "data = pd.DataFrame(data={\"n0\": tmp[:, 0], \"E0\": tmp[:, 1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Bayes' theorem\n",
    "\n",
    "Our analysis is based on Bayes' theorem. We need to define the prior, posterior, and likelihood. We use the (natural) logarithms of these quantities. The variables we will do the inference for are agglomerated in the parameter vector $\\theta = (\\mu_0, \\mu_1; \\Sigma_{0,0}, \\Sigma_{0,1}, \\Sigma_{1,1})$. To test the implementation, we define the $\\theta$ associated with the prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "theta_prior = (prior_params[\"mu\"][0], prior_params[\"mu\"][1],\n",
    "           prior_params[\"Psi\"][0, 0], prior_params[\"Psi\"][0, 1],\n",
    "           prior_params[\"Psi\"][1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True, fastmath=True)\n",
    "def validate_matrix(mat, raise_error=False, atol_sym=1e-08, atol_eval=1e-06):\n",
    "    \"\"\"\n",
    "    Checks that the 2x2 matrix `mat` is symmetric and positive definite\n",
    "    :param mat: 2x2 matrix\n",
    "    :param raise_error: raise error if not symmetric and positive definite\n",
    "    :param atol_sym: absolute tolerance used for comparison (symmetry)\n",
    "    :param atol_eval: absolute tolerance used for comparison (positiveness)\n",
    "    :return: returns boolean result of the validation\n",
    "    \"\"\"\n",
    "    stat_sym = np.abs(mat[0,1]-mat[1,0]) < atol_sym\n",
    "    #v, _ = np.linalg.eig(mat) # only `eig` supported by numba\n",
    "    eigvals = 0.5 * (mat[0,0] - np.array([1., -1]) * np.sqrt(4.* mat[0,1]*mat[1,0] + (mat[0,0] - mat[1,1])**2) + mat[1,1])\n",
    "    stat_pos_def = (eigvals > atol_eval).all()\n",
    "    stat = stat_sym and stat_pos_def\n",
    "    if not stat and raise_error:\n",
    "        raise ValueError(\"Non-symmetric and/or non-positive-definite 2x2 matrix encountered.\")\n",
    "    else:\n",
    "        return stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True)\n",
    "def parse_theta_vec(theta):\n",
    "    \"\"\"\n",
    "    converts the array/vector `theta` into a mean value and (symmetric) covariance matrix\n",
    "    :param theta: encodes mean vector (first two components) and the (symmetric) covariance matrix (last 3 components);\n",
    "    :return: mean vector, full covariance matrix\n",
    "    \"\"\"\n",
    "    mu = np.array([theta[0], theta[1]])\n",
    "    cov = np.array([[theta[2], theta[3]], [theta[3], theta[4]]])\n",
    "     \n",
    "    # One might think of using the Choleslky decomposition to enforce positive (semi-)definitess of the returned matrix\n",
    "    ## cov[0,1]=0 # treat components as lower triangle matrix\n",
    "    ## cov = cov @ cov.T # use Cholesky decomposition to ensure at at least positive semi-definiteness\n",
    "    # but this causes issues with the MLE and MAP solvers below\n",
    "    \n",
    "    return mu, cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def log_prior(theta):\n",
    "    \"\"\"\n",
    "    computes the log pdf of our chosen prior distribution, the normal-inverse-Wishart distribution\n",
    "    Note: we can't use here `numba` straightforwardly because it does not support the distribution functions; `numba-scipy` and `numba-stats` address this issue but the inverse Wishart distribution has not been implemented in these libraries\n",
    "    :param theta: parameter vector\n",
    "    :return: log pdf associated with the prior\n",
    "    \"\"\"\n",
    "    mu, sigma = parse_theta_vec(theta)\n",
    "    if not validate_matrix(sigma):\n",
    "        return -np.inf\n",
    "    log_norm = multivariate_normal.logpdf(x=mu, mean=prior_params[\"mu\"], cov=sigma/prior_params[\"kappa\"])\n",
    "    log_invwishart = invwishart.logpdf(x=sigma, df=prior_params[\"nu\"], scale=prior_params[\"Psi\"])\n",
    "    return log_norm + log_invwishart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def fast_log_2D_norm_invwishart(mu, Sigma, delta, gamma, Psi, alpha):\n",
    "    det_Sigma = -Sigma[0, 1] * Sigma[1, 0] + Sigma[0, 0] * Sigma[1, 1]\n",
    "    det_Psi = -Psi[0, 1] * Psi[1, 0] + Psi[0, 0] * Psi[1, 1]\n",
    "    Sigma_inv = np.array([[Sigma[1, 1], -Sigma[0, 1]], [-Sigma[1, 0], Sigma[0, 0]]]) / det_Sigma\n",
    "    invwishart_term = np.trace(Psi @ Sigma_inv)\n",
    "    diff = mu - delta\n",
    "    normal_term = gamma * np.dot(diff, np.dot(Sigma_inv, diff))\n",
    "    D = 2\n",
    "    norm_term = D / 2 * np.log(gamma)\n",
    "    norm_term += alpha / 2 * np.log(det_Psi)\n",
    "    norm_term -= (alpha + D + 2) / 2 * np.log(det_Sigma)\n",
    "    norm_term -= D / 2 * np.log(2. * np.pi)\n",
    "    norm_term -= alpha * D / 2 * np.log(2)\n",
    "    alpha_half = alpha / 2\n",
    "    norm_term -= 0.5 * np.log(np.pi) + np.log(math.gamma(alpha_half)) + np.log(math.gamma(alpha_half - 0.5))\n",
    "    return norm_term - 0.5 * (invwishart_term + normal_term)\n",
    "\n",
    "# def log_prior(theta):\n",
    "#     mu, sigma = parse_theta_vec(theta)\n",
    "#     if not validate_matrix(sigma):\n",
    "#         return -np.inf\n",
    "#     return fast_log_2D_norm_invwishart(mu=mu, Sigma=sigma, delta=prior_params[\"mu\"],\n",
    "#                                        gamma=prior_params[\"kappa\"], Psi=prior_params[\"Psi\"], alpha=prior_params[\"nu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True, fastmath=True)\n",
    "def log_likelihood(theta, sat_data):\n",
    "    \"\"\"\n",
    "    computes the logarithm of the likelihood function, the standard Chi-squared likelihood function\n",
    "    :param theta: parameter vector\n",
    "    :param sat_data: array (number of point, 2) containing the datasets for the saturation density and energy\n",
    "    :return: logarithm of the likelihood function\n",
    "    \"\"\"\n",
    "    mu, sigma = parse_theta_vec(theta)\n",
    "    # exclude non-positive-definite covariance matrices\n",
    "    if not validate_matrix(sigma):\n",
    "        #print('covariance not valid')\n",
    "        return -np.inf\n",
    "    chi2 = 0.\n",
    "    sigma_inv = np.linalg.inv(sigma)\n",
    "    for row in sat_data:\n",
    "        diff = (row - mu)\n",
    "        chi2 += np.dot(diff, sigma_inv @ diff)\n",
    "    return -0.5 * (chi2 + len(sat_data) * np.log(np.abs(np.linalg.det(sigma))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def log_posterior(theta, sat_data):\n",
    "    \"\"\"\n",
    "    computes the log posterior, which is in this particular case also the normal-inverse-Wishart distribution (conjugagy).\n",
    "    :param theta: parameter vector\n",
    "    :param sat_data: pandas dataframe containing the datasets for the saturation density and energy\n",
    "    :return: log pdf associated with the posterior\n",
    "    \"\"\"\n",
    "    lp = log_prior(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + log_likelihood(theta, sat_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Testing our implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Bayesian inference\n",
    "\n",
    "### Determine initial walker position using MLE or MAP\n",
    "\n",
    "We need to specify the initial position of the random walkers for Monte Carlo sampling. We use here either the MAP or MLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_initial_walker_pos(function=log_posterior, method=\"Nelder-Mead\", return_theta=False, **kwargs):\n",
    "    \"\"\"\n",
    "    finds the maximum of the argument `function`; to be used to determine either the maximum a posterior (MAP) or maximum likelihood estimation (MLE)\n",
    "    :param function: either likelihood function or posterior or prior\n",
    "    :param method: method used for optimization, see scipy documentation\n",
    "    :param kwargs: options passed to the optimizer, see scipy documentation\n",
    "    :param return_theta: boolean to request solution vector or (mu, cov)\n",
    "    :return: mean value and covariance associated with the found maximum or found maximum (if return_theta)\n",
    "    \"\"\"\n",
    "    nll = lambda *args: -function(*args)\n",
    "    args = () if function == log_prior else (data.to_numpy(),)\n",
    "    sol = minimize(nll, theta_prior, args=args, method=method, tol=1e-12, options=kwargs)\n",
    "    mu, cov = parse_theta_vec(sol.x)\n",
    "    if not validate_matrix(cov):\n",
    "        raise ValueError(\"Covariance matrix not positive definite\")\n",
    "    return sol.x if return_theta else (mu, cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "map_estimate = get_initial_walker_pos(function=log_posterior, method=\"Nelder-Mead\", maxiter=10000000)\n",
    "mle_estimate = get_initial_walker_pos(function=log_likelihood, method=\"SLSQP\", maxiter=10000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We expect both the MAP and MLE to be close to the (input) prior, so let's compute the difference of the respective mean values and covariance matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map\n",
      "[0.00056925 0.01060897]\n",
      "[[ 2.19399125e-06  8.00464643e-06]\n",
      " [ 8.00464643e-06 -8.45639078e-03]]\n",
      "estimated covariance matrix checks out: True\n",
      "mle\n",
      "[0.0005697  0.01061973]\n",
      "[[ 2.90563271e-06  8.08717359e-06]\n",
      " [ 8.08717359e-06 -7.80745770e-03]]\n",
      "estimated covariance matrix checks out: True\n"
     ]
    }
   ],
   "source": [
    "for lbl, est in ((\"map\", map_estimate), (\"mle\", mle_estimate)):\n",
    "    print(lbl)\n",
    "    mu, cov = est\n",
    "    print(mu-prior_params[\"mu\"])\n",
    "    print(cov-prior_params[\"Psi\"])\n",
    "    print(\"estimated covariance matrix checks out:\", validate_matrix(cov))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since the prior is quite uninformed, the MLE and MAP are very close to each other, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Monte Carlo sampling\n",
    "\n",
    "We use the library `multiprocess` because `multiprocessing` has issues with Jupyter notebooks.\n",
    "\n",
    "Warning: runtime might be several hours depending on the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_init_pos_walkers(init_pos, nwalkers, enforce_posdef=True, std_perturbation=1e-2, delta_mean=0.02):\n",
    "    \"\"\"\n",
    "    creates initial positions of `nwalkers` by randomly perturbing a given (single) initial position `init_pos` \n",
    "    :param init_pos: datasets for saturation point\n",
    "    :param nwalkers: number of walkers requested\n",
    "    :param enforce_posdef: enforce that matrix component in the output is positive (semi-)definite (in addition to symmetric)\n",
    "    :param std_perturbation: standard deviation of the perturbation\n",
    "    :param delta_mean: mean value of the normal distribution (delta_mean, std_perturbation**2) for the minimum eigenvalues used to obtain the nearest pos. def. matrix in Frobenius norm\n",
    "    :return: array of shape (nwalkers, len(init_pos)) containing the perturbed initial positions of the random walkers\n",
    "    \"\"\"\n",
    "    # perturbation preserves symmetry but not definiteness of matrix `init_pos`\n",
    "    # return array will be component-wise distributed as \"Normal(mu=init_pos, sigma^2=std_perturbation)\"\n",
    "    ret = init_pos + std_perturbation * np.random.randn(nwalkers, len(init_pos)) \n",
    "    \n",
    "    # make matrix encoded in `init_pos` (i.e., theta) positive (semi-)definite after perturbation,\n",
    "    # by finding the nearest sym. pos. def. matrix in Frobenius norm\n",
    "    # https://nhigham.com/2021/01/26/what-is-the-nearest-positive-semidefinite-matrix/ (based on Cheng & Higham, 1998)\n",
    "    if enforce_posdef:\n",
    "        triu_indices = np.triu_indices(2)\n",
    "        for row in ret:\n",
    "            mu, mat = parse_theta_vec(row)\n",
    "            if not validate_matrix(mat): # leave walker alone if its matrix is already pos. def.\n",
    "                v, w = np.linalg.eig(mat)\n",
    "                delta = np.abs(delta_mean+std_perturbation * np.random.randn())\n",
    "                cov = w @ np.diag(np.clip(v, a_min=delta, a_max=None)) @ w.T  # increase eigenvalues if necessary\n",
    "                validate_matrix(cov, raise_error=True)\n",
    "                row[2:] = cov[triu_indices]\n",
    "    return ret\n",
    "# perturb_init_pos_walkers([1,1,2,3,4], 5)\n",
    "# perturb_init_pos_walkers([1,1,1,0,0.001], 5, enforce_posdef=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocess as mp\n",
    "def run_mcmc_sampler(sat_data, nwalkers=1000, nsteps=50, nsteps_burn=50, log_prob_fn=log_posterior, num_threads=4, offset=1e-2):\n",
    "    \"\"\"\n",
    "    samples the function provided by `log_prob_fn`\n",
    "    :param sat_data: datasets for saturation point\n",
    "    :param nwalkers: number of walkers\n",
    "    :param nsteps: number of MCMC steps after burn-in phase\n",
    "    :param nsteps_burn: number of MCMC steps used for burn in\n",
    "    :param log_prob_fn: function to be sampled\n",
    "    :param num_threads: number of threads to be used\n",
    "    :param offset: magnitude of random displacement from walkers' initial positions\n",
    "    :return: sampler, pos, prob, state (see emcee documentation)\n",
    "    \"\"\"\n",
    "    if log_prob_fn == log_likelihood:\n",
    "        init_pos = get_initial_walker_pos(function=log_prob_fn, method=\"SLSQP\",\n",
    "        return_theta=True)\n",
    "    else:\n",
    "        init_pos = get_initial_walker_pos(function=log_posterior, method=\"Nelder-Mead\", return_theta=True)\n",
    "\n",
    "    if num_threads is None:\n",
    "        num_threads = mp.cpu_count()\n",
    "\n",
    "    ndim = len(init_pos)\n",
    "    args = None if log_prob_fn == log_prior else (sat_data.to_numpy(),)\n",
    "    with mp.Pool(num_threads) as pool:\n",
    "        # for multiprocessing with emcee, see https://emcee.readthedocs.io/en/stable/tutorials/parallel/#multiprocessing\n",
    "        # https://stackoverflow.com/questions/41385708/multiprocessing-example-giving-attributeerror\n",
    "        sampler = mc.EnsembleSampler(nwalkers=nwalkers, ndim=ndim,\n",
    "                                     log_prob_fn=log_prob_fn, args=args, pool=pool)\n",
    "        \n",
    "        # let each walker start at a slightly different initial position (random perturbation)\n",
    "        pos = perturb_init_pos_walkers(init_pos, nwalkers, enforce_posdef=True, std_perturbation=offset)\n",
    "    \n",
    "        if nsteps_burn > 0:\n",
    "            pos, _, _ = sampler.run_mcmc(initial_state=pos, nsteps=nsteps_burn, progress=True)\n",
    "            sampler.reset()\n",
    "        pos, prob, state = sampler.run_mcmc(initial_state=pos, nsteps=nsteps, progress=True)\n",
    "    return sampler, pos, prob, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Testing\n",
    "\n",
    "We test here that the implemented distribution functions match the analytically known results for the mean values.\n",
    "\n",
    "We begin the testing with the **prior distribution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50000/50000 [02:52<00:00, 290.23it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500000/500000 [28:46<00:00, 289.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.59886773e-01, -1.59001510e+01,  9.73476476e-05, -4.43049593e-05,\n",
       "        9.80986545e-02])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler, pos, prob, state = run_mcmc_sampler(sat_data=data, log_prob_fn=log_prior, nwalkers=10, nsteps=500000, nsteps_burn=50000)\n",
    "samples_prior = sampler.flatchain\n",
    "# sampler.chain[:,:,4].mean(axis=1)\n",
    "samples_prior.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The expected covariance of the two components of the **mean vector** is $\\Psi / \\kappa$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.54832220e-06, -1.10223667e-05],\n",
       "       [-1.10223667e-05, -7.80559534e-03]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#samples#.mean(axis=0)\n",
    "expected = prior_params[\"Psi\"]/prior_params[\"kappa\"]\n",
    "got = np.cov(samples_prior[:,0], samples_prior[:,1])\n",
    "got - expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Our expectation for the mean values from the **prior distribution** is a follows:\n",
    "$$\n",
    "E(\\mu) = \\mu_0 \\;, \\qquad\n",
    "E(\\Sigma) = \\frac{\\Sigma}{\\nu - p -1} \\quad \\mathrm{for} \\; \\nu > p + 1\n",
    "$$\n",
    "with $p=2$ being associated with rank of the $p \\times p$ covariance matrix $\\Sigma$. Hence, the mean value of the covariance matrix is _undefined_ in the original case we considered, $\\nu_0 = 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def mean_val_inv_wish(Psi, nu):\n",
    "    \"\"\"\n",
    "    computes the mean value of the Inverse-Wishart distribution, see https://en.wikipedia.org/wiki/Inverse-Wishart_distribution\n",
    "    :param Lambda: covariance matrix\n",
    "    :param nu: degree of freedom\n",
    "    :return: mean value of the associated Inverse-Wishart distribution\n",
    "    \"\"\"\n",
    "    p = len(Psi)\n",
    "    mean = Psi / (nu - p - 1) if nu > p+1 else np.inf\n",
    "    mode = Psi / (nu + p + 1)\n",
    "\n",
    "    tmp = nu - p - 1\n",
    "    cov = np.zeros((4,4))\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ii = i + 2*j\n",
    "            for k in range(2):\n",
    "                for l in range(2):\n",
    "                    jj = k + 2*l\n",
    "                    cov[ii, jj] = 2 * Psi[i,j]*Psi[k,l] + tmp * (Psi[i,k]*Psi[j,l] + Psi[i,l]*Psi[k,j])\n",
    "                    # taken from https://en.wikipedia.org/wiki/Inverse-Wishart_distribution#Moments\n",
    "    cov = cov / (nu-p)*tmp**2*(nu-p-3)\n",
    "    print(\"covariance matrix\", cov)\n",
    "    return mean, mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariance matrix [[-2.000000e-08 -0.000000e+00 -0.000000e+00 -1.024000e-05]\n",
      " [-0.000000e+00 -5.120000e-06 -5.120000e-06 -0.000000e+00]\n",
      " [-0.000000e+00 -5.120000e-06 -5.120000e-06 -0.000000e+00]\n",
      " [-1.024000e-05 -0.000000e+00 -0.000000e+00 -2.097152e-02]]\n",
      "mean of mean vector: [  0.16 -15.9 ]\n",
      "mean of covariance matrix: [[1.000e-04 0.000e+00]\n",
      " [0.000e+00 1.024e-01]]\n",
      "mode of covariance matrix: [[1.42857143e-05 0.00000000e+00]\n",
      " [0.00000000e+00 1.46285714e-02]]\n"
     ]
    }
   ],
   "source": [
    "mean_mean_vec = prior_params[\"mu\"]\n",
    "mean_cov, mode_cov = mean_val_inv_wish(prior_params[\"Psi\"], prior_params[\"nu\"])\n",
    "print(f\"mean of mean vector: {mean_mean_vec}\")\n",
    "print(f\"mean of covariance matrix: {mean_cov}\")\n",
    "print(f\"mode of covariance matrix: {mode_cov}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The mean vector and covariance matrix seem to be in agreement with the ones from the prior.\n",
    "Let's investigate the mode of the prior, which seems slightly off. Why? Because the prior is relatively flat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0.16, -15.9 ]),\n",
       " array([[ 1.24999999e-05, -1.51130086e-11],\n",
       "        [-1.51130086e-11,  1.27999999e-02]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_initial_walker_pos(function=log_prior, method=\"Nelder-Mead\", maxiter=10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.35339508e-07, -1.05874264e-06,  1.09300518e-04],\n",
       "       [-1.05874264e-06,  1.13970375e-04,  8.44886242e-04],\n",
       "       [ 1.09300518e-04,  8.44886242e-04,  1.99177595e-01]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov((samples_prior[:,2], samples_prior[:,3], samples_prior[:,4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Why is the last component so large?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, let's investigate the predictive $y$ distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predictive_y(samples, size=1):\n",
    "    y_samples = []\n",
    "    for row in samples:\n",
    "        mu, Sigma = parse_theta_vec(row)\n",
    "        y_samples.append(multivariate_normal.rvs(mean=mu, cov=Sigma, size=size))\n",
    "    return np.array(y_samples)\n",
    "pred_y_samples = predictive_y(samples_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For the mean value of the prior predictive $y$, which is the Student's $t$-distribution, we expect just the prior mean vector. We find agreement with the expectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.15988742, -15.89994685])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y_samples.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We expect\n",
    "\\begin{equation}\n",
    "    P(\\boldsymbol{y}^*| \\mathcal{D}) \\sim t_{\\nu_n -p +1} \\Big( \\boldsymbol{\\mu_n}, \\frac{\\boldsymbol{\\Lambda_n}(k_n+1)}{k_n(\\nu_n-p+1)} \\Big),\n",
    "\\end{equation}\n",
    "so the mean vector is just the mean vector of the prior and the variance\n",
    "$$\n",
    "\\frac{\\nu'_n}{\\nu'_n-2} \\boldsymbol{\\Sigma}' =  \\frac{\\boldsymbol{\\Lambda_n}(k_n+1)}{k_n(\\nu_n-p-1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.000e-04, 0.000e+00],\n",
       "       [0.000e+00, 2.048e-01]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def variance_multivar_t(nu, Sigma):\n",
    "    # see https://en.wikipedia.org/wiki/Multivariate_t-distribution\n",
    "    return nu/(nu-2) * Sigma if nu > 2 else np.inf\n",
    "variance_multivar_t(prior_params[\"nu\"]-2+1,\n",
    "                    prior_params[\"Psi\"] * (prior_params[\"kappa\"]+1)/(prior_params[\"kappa\"]*(prior_params[\"nu\"]-2+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.90338994e-04, -6.00375048e-05],\n",
       "       [-6.00375048e-05,  1.92650132e-01]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov(pred_y_samples[:,0], pred_y_samples[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It looks like the variances agree (although not perfectly). Why? Do we just need more sampling points? Prior too uninformed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, let's consider the **posterior distribution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [06:41<00:00, 248.83it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300000/300000 [20:10<00:00, 247.87it/s]\n"
     ]
    }
   ],
   "source": [
    "sampler, pos, prob, state = run_mcmc_sampler(sat_data=data, log_prob_fn=log_posterior, nwalkers=10, nsteps=300000, nsteps_burn=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.60568014e-01, -1.58894218e+01,  1.02876388e-04,  7.20937185e-06,\n",
       "        9.46359550e-02])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_posterior = sampler.flatchain\n",
    "samples_posterior.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Our expectation for the mean values from the updated prior distribution due to conjugacy is as follows (see our manuscript for the equations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected mean vector [  0.16056925 -15.88939104]\n",
      "covariance matrix [[2.12439583e+07 1.66399881e+06 1.66399881e+06 1.96201072e+07]\n",
      " [1.66399881e+06 9.75475991e+09 9.75475991e+09 1.52965993e+09]\n",
      " [1.66399881e+06 9.75475991e+09 9.75475991e+09 1.52965993e+09]\n",
      " [1.96201072e+07 1.52965993e+09 1.52965993e+09 1.79522629e+13]]\n",
      "expected covariance matrix: [[1.02908636e-04 8.06063757e-06]\n",
      " [8.06063757e-06 9.46005591e-02]]\n"
     ]
    }
   ],
   "source": [
    "ret = dict()\n",
    "ret[\"kappa\"] = prior_params[\"kappa\"] + len(data)\n",
    "ret[\"nu\"] = prior_params[\"nu\"] + len(data)\n",
    "ret[\"sample_mean\"] = data.mean().to_numpy()\n",
    "ret[\"mu\"] = np.array([prior_params[\"kappa\"], len(data)]) @ np.array([prior_params[\"mu\"], ret[\"sample_mean\"]]) / ret[\"kappa\"]\n",
    "diff = ret[\"sample_mean\"] - prior_params[\"mu\"]\n",
    "tmp = data - ret[\"sample_mean\"]\n",
    "ret[\"sum_squared_dev\"] = np.sum([np.outer(row.to_numpy(), row.to_numpy()) for irow, row in tmp.iterrows()], axis=0)\n",
    "ret[\"Psi\"] = prior_params[\"Psi\"] + ret[\"sum_squared_dev\"] + (prior_params[\"kappa\"] * len(data) / ret[\"kappa\"]) * np.outer(diff, diff)\n",
    "print(\"expected mean vector\", ret[\"mu\"])\n",
    "print(\"expected covariance matrix:\", mean_val_inv_wish(ret[\"Psi\"], ret[\"nu\"])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that $\\Sigma_{1,1}$ (lower right) is large \\[Note that the matrix elements above include the denominator from the mean value\\]. Does that make sense? But keep in mind that $\\Sigma/\\kappa_n$ will be used as covariance matrix in the normal distribution (of the Normal-Inverse Wishart)---but not in the inverse Wishart distribution.\n",
    "\n",
    "The mean vector looks like a match. Except for the $\\Sigma_{11}$ component, it does not look like a good match for $\\Sigma$. Here are the ratios of the matrix elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariance matrix [[2.12439583e+07 1.66399881e+06 1.66399881e+06 1.96201072e+07]\n",
      " [1.66399881e+06 9.75475991e+09 9.75475991e+09 1.52965993e+09]\n",
      " [1.66399881e+06 9.75475991e+09 9.75475991e+09 1.52965993e+09]\n",
      " [1.96201072e+07 1.52965993e+09 1.52965993e+09 1.79522629e+13]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.99968663, 0.89439226],\n",
       "       [0.89439226, 1.00037416]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m, c = parse_theta_vec(samples_posterior.mean(axis=0))\n",
    "c/mean_val_inv_wish(ret[\"Psi\"], ret[\"nu\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.02806687e-04, -8.05136233e-06],\n",
       "       [-8.05136233e-06, -9.45062345e-02]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#samples#.mean(axis=0)\n",
    "expected = ret[\"Psi\"]/ret[\"kappa\"]\n",
    "got = np.cov(samples_posterior[:,0], samples_posterior[:,1])\n",
    "got - expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariance matrix [[2.12439583e+07 1.66399881e+06 1.66399881e+06 1.96201072e+07]\n",
      " [1.66399881e+06 9.75475991e+09 9.75475991e+09 1.52965993e+09]\n",
      " [1.66399881e+06 9.75475991e+09 9.75475991e+09 1.52965993e+09]\n",
      " [1.96201072e+07 1.52965993e+09 1.52965993e+09 1.79522629e+13]]\n",
      "mean of mean vector: [  0.16056925 -15.88939104]\n",
      "mean of covariance matrix: [[1.02908636e-04 8.06063757e-06]\n",
      " [8.06063757e-06 9.46005591e-02]]\n",
      "mode of covariance matrix: [[1.02295477e-04 8.01260993e-06]\n",
      " [8.01260993e-06 9.40369013e-02]]\n"
     ]
    }
   ],
   "source": [
    "mean_mean_vec = ret[\"mu\"]\n",
    "mean_cov, mode_cov = mean_val_inv_wish(ret[\"Psi\"], ret[\"nu\"])\n",
    "print(f\"mean of mean vector: {mean_mean_vec}\")\n",
    "print(f\"mean of covariance matrix: {mean_cov}\")\n",
    "print(f\"mode of covariance matrix: {mode_cov}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0.16056925, -15.88939103]),\n",
       " array([[1.02193991e-04, 8.00464643e-06],\n",
       "        [8.00464643e-06, 9.39436092e-02]]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_initial_walker_pos(function=log_posterior, method=\"Nelder-Mead\", maxiter=10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.12743303e-11,  1.20431701e-12, -1.69868019e-11],\n",
       "       [ 1.20431701e-12,  9.69135390e-09,  4.44546947e-11],\n",
       "       [-1.69868019e-11,  4.44546947e-11,  1.79606406e-05]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov((samples_posterior[:, 2], samples_posterior[:, 3], samples_posterior[:, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.75536795e-06,  1.71730660e-04])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y_samples = predictive_y(samples_posterior)\n",
    "pred_y_samples.mean(axis=0) - ret[\"mu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.03011442e-04, 8.06869015e-06],\n",
       "       [8.06869015e-06, 9.46950651e-02]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variance_multivar_t(ret[\"nu\"] - 2 + 1,\n",
    "                    ret[\"Psi\"] * (ret[\"kappa\"] + 1) / (ret[\"kappa\"] * (ret[\"nu\"] - 2 + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.03115102e-04, 6.06422152e-06],\n",
       "       [6.06422152e-06, 9.46662934e-02]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov(pred_y_samples[:, 0], pred_y_samples[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Useful links\n",
    "\n",
    "* [Covariances and Pearson correlation](https://cxwangyi.wordpress.com/2010/08/29/pearson-correlation-coefficient-covariance-matrix-and-linear-dependency/)\n",
    "* [emcee tutorial for fitting (1)](https://prappleizer.github.io/Tutorials/MCMC/MCMC_Tutorial_Solution.html)\n",
    "* [emcee tutorial for fitting (2)](https://users.obs.carnegiescience.edu/cburns/ipynbs/Emcee.html)\n",
    "* [emcee tutorial for fitting (3)](https://emcee.readthedocs.io/en/stable/tutorials/line/)\n",
    "* [multivariate normal distribution](https://online.stat.psu.edu/stat505/book/export/html/636)\n",
    "* [A Note on Wishart and Inverse Wishart Priors for Covariance Matrix](https://jbds.isdsa.org/public/journals/1/html/v1n2/p2/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kappa': 1001,\n",
       " 'nu': 1004,\n",
       " 'mu': array([  0.16056925, -15.88939104]),\n",
       " 'Psi': array([[1.03011545e-01, 8.06869820e-03],\n",
       "        [8.06869820e-03, 9.46951596e+01]])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modules.StatisticalModel import StatisticalModel\n",
    "conjp_model = StatisticalModel(data, quantities=[\"n0\", \"E0\"], prior_params=prior_params)\n",
    "conjp_model.posterior_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sat",
   "language": "python",
   "name": "sat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
